import pandas as pd, numpy as np
import argparse
import pickle
import ast
import numpy as np
from sklearn.metrics import f1_score
import os
import re

PATH_TO_OG_CHEXPERT_CSV = os.path.join(os.getcwd(), 'ManyICL', 'dataset', 'CheXpert', 'chexpert_test_df_labels.csv')

def accuracy_score(y_true, y_pred):
    assert len(y_true) == len(y_pred)
    return np.sum(y_true == y_pred) / len(y_true)

def accuracy_and_F1(test_df, pred_df, bootstrap):
    # Accuracy + F1 calculations
    all_ori_accs = []
    all_std_devs = []
    all_f1s = []
    all_f1_std_devs = []

    for class_name in classes:
        y_true = test_df[class_name].values
        y_pred = pred_df[class_name].values

        ori_acc = 100 * accuracy_score(y_true, y_pred)
        accs = []
        for boot in range(bootstrap):
            idx = np.random.choice(len(y_true), size=len(y_true), replace=True)
            accs.append(100 * accuracy_score(y_true[idx], y_pred[idx]))
        std_dev = np.std(accs)
        print(f"{class_name.upper()} Accuracy: {ori_acc:.2f} +- {std_dev:.2f}")

        ori_f1 = f1_score(y_true, y_pred, zero_division=0)
        f1s = []
        for boot in range(bootstrap):
            idx = np.random.choice(len(y_true), size=len(y_true), replace=True)
            y_true_boot = y_true[idx]
            y_pred_boot = y_pred[idx]
            f1 = f1_score(y_true_boot, y_pred_boot, zero_division=0)
            f1s.append(f1)
        f1_std_dev = np.std(f1s)
        print(f"{class_name.upper()} F1 score: {ori_f1:.2f} +- {f1_std_dev:.2f}\n")

        all_ori_accs.append(ori_acc)
        all_std_devs.append(std_dev)
        all_f1s.append(ori_f1)
        all_f1_std_devs.append(f1_std_dev)
    
    print(f"Averaged Accuracy: {np.mean(all_ori_accs):.2f} +- {np.mean(all_std_devs):.2f}")
    print(f"Macro Averaged F1: {np.mean(all_f1s):.2f} +- {np.mean(all_f1_std_devs):.2f}")

def convert_pkl(raw_pickle, sanity_df, answer_prefix="Answer Choice "):
    """
    Convert raw pickle files saved in run.py to answers to each individual question

    raw_pickle [dict]: Python dict read from raw pickle files
    answer_prefix [str]: Prefix for answer matching, it should match the template in prompt.py
    """
    results = {}
    def extract_response(text, pattern):
        match = re.search(pattern, text, re.DOTALL)
        if match == None:
            match == text
        else:
            match = match.group()
        return match

    def extract_ans(ans_str, search_substring):
        # Split the string into lines
        lines = ans_str.split("\n")

        for line in lines:
            # Check if the line starts with the specified substring
            if line.startswith(search_substring):
                # If it does, add it to the list of extracted rows
                return line[len(search_substring) :].strip()
        return "ERROR"  # Answer not found

    for k, v in raw_pickle.items():
        if k.startswith("["):  # Skip token_usage
            qns_idx = ast.literal_eval(k)
            for idx, qn_idx in enumerate(qns_idx):
                parsed_answer = extract_ans(
                    v, f"{answer_prefix}{idx+1}:"
                )  # We start with question 1
                results[qn_idx] = parsed_answer
                pattern = fr'--BEGIN FORMAT TEMPLATE FOR QUESTION {idx+1}---(.*?)---END FORMAT TEMPLATE FOR QUESTION {idx+1}---'
                sanity_df.loc[qn_idx, 'raw_response'] = extract_response(v, pattern)
                sanity_df.loc[qn_idx, 'parsed_answer'] = parsed_answer
    return results


def cal_metrics(
    EXP_NAME, test_df, classes, class_desp, show_error=True, bootstrap=1000
):
    """
    Calculate accuracy from model responses

    EXP_NAME [str]: experiment name which should match the pkl file generated by run.py
    test_df [pandas dataframe]: dataframe for test cases
    classes [list of str]: names of categories for classification, and this should match tbe columns of test_df and demo_df.
    class_desp [list of str]: category descriptions for classification, and these are the actual options sent to the model
    show_error [bool]: whether parsing errors will be printed
    bootstrap [int]: number of replicates for bootstrapping standard deviation
    """
    # Create a new DataFrame with only empty'response' and 'parsed_answer' columns
    sanity_df = pd.DataFrame(index=test_df.index, columns=['raw_response', 'parsed_answer'])

    with open(f"{EXP_NAME}.pkl", "rb") as f:
        results = pickle.load(f)
    results = convert_pkl(
        results, sanity_df
    )  # Convert the batched results into individual answers
    sanity_df.to_csv(f'{EXP_NAME}_sanity.csv')

    # initialize predictive dafaframe with zeros
    pred_df = test_df.copy()
    pred_df[classes] = 0

    num_errors = 0
    labels, preds = [], []
    for i in test_df.itertuples():
        if (i.Index not in results) or (results[i.Index].startswith("ERROR")):
            num_errors += 1
            if show_error:
                print(i.Index, f"answer not found")
            continue

        pred_text = results[i.Index]

        for class_name in class_desp:
            if class_name.lower() in pred_text.lower():
                pred_df.loc[i.Index, class_name] = 1

    pred_df.to_csv(f'{EXP_NAME}_prediction.csv')

    print("Accuracy And F1 For Total Dataset:")
    accuracy_and_F1(test_df, pred_df, bootstrap)
    print("----------\n")


    # results by demographic
    white_labels = ['White','White, non-Hispanic','White or Caucasian']
    black_labels = ['Black or African American','Black, non-Hispanic']
    df = pd.read_csv(PATH_TO_OG_CHEXPERT_CSV)

    black_rows = []
    white_rows = []
    for i in test_df.itertuples():
        row = df[df['updated_path'].str.endswith(i.Index)]
        if row['race'].values[0] in black_labels:
            black_rows.append(i.Index)
        elif row['race'].values[0] in white_labels:
            white_rows.append(i.Index)

    if (len(white_rows) + len(black_rows) != len(test_df)):
        print("----------")
        print(f"Only counted {len(white_rows) + len(black_rows)} race labels out of {len(test_df)}")
        print("----------")

    if black_rows:
        black_test = test_df.loc[black_rows].copy()
        black_pred = pred_df.loc[black_rows].copy()
    if white_rows:
        white_test = test_df.loc[white_rows].copy()
        white_pred = pred_df.loc[white_rows].copy()

    if black_rows:
        print("Accuracy And F1 For Given Black Race Condition:")
        accuracy_and_F1(black_test, black_pred, bootstrap)
        print("----------\n")
    if white_rows:    
        print("Accuracy And F1 For Given White Race Condition:")
        accuracy_and_F1(white_test, white_pred, bootstrap)
        print("----------\n")


if __name__ == "__main__":
    # Initialize the parser
    parser = argparse.ArgumentParser(description="Experiment script.")
    # Adding the arguments
    parser.add_argument(
        "--dataset",
        type=str,
        required=True,
        default="UCMerced",
        help="The dataset to use",
    )
    parser.add_argument(
        "--model",
        type=str,
        required=False,
        default="Gemini1.5",
        help="The model to use",
    )
    parser.add_argument(
        "--location",
        type=str,
        required=False,
        default="us-central1",
        help="The location for the experiment",
    )
    parser.add_argument(
        "--num_shot_per_class",
        type=int,
        required=True,
        help="The number of shots per class",
    )
    parser.add_argument(
        "--num_qns_per_round",
        type=int,
        required=False,
        default=1,
        help="The number of questions asked each time",
    )

    # Parsing the arguments
    args = parser.parse_args()

    # Using the arguments
    dataset_name = args.dataset
    model = args.model
    location = args.location
    num_shot_per_class = args.num_shot_per_class
    num_qns_per_round = args.num_qns_per_round
    
    # dataset_name = "CheXpert"
    # model = "gpt-4o"
    # location = "us-central1"
    # num_shot_per_class = 0
    # num_qns_per_round = 1

    # Read the two dataframes for the dataset
    demo_df = pd.read_csv(f"ManyICL/dataset/{dataset_name}/demo.csv", index_col=0)
    test_df = pd.read_csv(f"ManyICL/dataset/{dataset_name}/test.csv", index_col=0)

    classes = list(test_df.columns)  # classes for classification
    class_desp = classes  # The actual list of options given to the model. If the column names are informative enough, we can just use them.
    class_to_idx = {class_name: idx for idx, class_name in enumerate(classes)}

    EXP_NAME = f"{dataset_name}_{num_shot_per_class*len(classes)}shot_{model}_{num_qns_per_round}"
    cal_metrics(EXP_NAME, test_df, classes, class_desp)
